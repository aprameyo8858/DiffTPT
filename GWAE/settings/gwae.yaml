model_name: gwae
hyperparameters:
  lr: 1e-4
  z_dim: 64
  coef_w: 1
  coef_d: 1
  coef_qentropy: 0.03
  sampler_type: neural
  merged_condition: true
  lr_disc: 1e-4
  coef_gp: 10
  times_d_training: 5
  batchnorm: false
  resblock: false
  epochs: 20    #not present before
  distance_coef_initial: 2.5
dataset: mnist  #shapes3d       #omniglot  #cifar10    #celeba  #mnist
logger_path: runs/ebsgw/mnist_gwae_1   #runs/celeba_gwae
batch_size: 64   #default is 64
cuda_sync: false
very_verbose: true
until_convergence: true #it was true originally
patience: 10
